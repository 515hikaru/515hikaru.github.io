<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Data Preprocessing on i was perfect</title>
    <link>tech.515hikaru.net/categories/data-preprocessing/</link>
    <description>Recent content in Data Preprocessing on i was perfect</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>ja-jp</language>
    <lastBuildDate>Sat, 09 Dec 2017 00:00:00 +0900</lastBuildDate>
    
	<atom:link href="tech.515hikaru.net/categories/data-preprocessing/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>なぜデータの前処理は難しいのか</title>
      <link>tech.515hikaru.net/post/2017/12/09/why-data-preprocessing-is-difficult/</link>
      <pubDate>Sat, 09 Dec 2017 00:00:00 +0900</pubDate>
      
      <guid>tech.515hikaru.net/post/2017/12/09/why-data-preprocessing-is-difficult/</guid>
      <description>データ分析の業務工程のうち、半分以上は前処理で占められているという話がある。おそらく業界の人間であれば必ず一度は聞き、実際に実感する話であり、それ以外の人でも興味がある人であればこの話を聞いたことがあるだろう。
しかし実際のところなぜデータの前処理でそこまで時間を食うのか、あるいは何がそんなに難しいのかを解説されたことがほとんどない気がする1。ということで、1年ほど仕事をしてきた私見を書いてみようと思う。あくまでも個人の経験と偏見に基づいた話なのでこれが「普遍的な難しさである」と主張する気はない。しかし、なんとなく「似たようなこと」は日本中のそこかしこで起きているんじゃないかと根拠もなく思ってはいる。
この記事は個人の見解であり、筆者が所属する組織・団体などの意見を代表するものではありません2。
前提 繰り返しになるが、データの前処理で筆者がよく難しいと思うことを思いつく順に書いていこうと思う。
ただ、本論に入る前に少しだけ筆者のバックグラウンドを説明しておく。筆者は基本的にはエクセルシートやcsvファイルを受け取って、そのデータを用いて予測モデルを作ったり、先日も記事にした類似度計算をすることでリコメンドシステムを実装したりするのが仕事である。どの業務をする上でも、csvファイルから必要な列のみ取り出したり、データの形式を変えたりするなど前処理のプロセスは必ず生じる。また、画像の前処理の経験はなく、テキストデータの前処理しか経験がない。
データの不備の検知しづらさ 少し想像してみてほしい。Excelファイルに10,000人分の身長が記録されていたとする。このとき、身長が [cm] という単位で記録されるべきところ、入力者が誤って [mm] で入力してしまっている。どうやって探すだろうか?
予め「このデータには絶対に上記のような不備がある」とわかっているのであればそれを発見するのはそこまで難しい話ではない。大きい順、あるいは小さい順にソートして人間としては(cmでの数値だとしたとき)身長が高すぎる/低すぎるものを見つければいい。可視化して明らかなハズレ値を調べるというのも手だろう。
しかし、残念ながら経験上いきなりデータの不備を疑うことは稀だ。時間に追われていることも多いし、いきなり前処理のプログラムを書いてしまうだろう。データベースに登録したり、あるいは別のcsvファイルと結合したりする。プログラムは異常な値を含んでいても「回ってしまうから」だ。
時間が十分にあれば事前のチェックを時間をとってできるかもしれないが、時間がない場合は見逃されてしまうだろう。上の例では身長のみだったので外れ値を探すのは難しくなかったかもしれないが、身長、体重、年齢、血液型、住んでいる都道府県などなどデータが数十、数百あった場合。異常な値をいかにみつけるか?すべて手作業でやるしかないのなら、その処理がそのプロジェクトの成功にどれほど寄与するだろうか?
データの変更によるプログラムの変更 データその1を前処理したスクリプトで、データその2を前処理することがよくある。最初から大量のデータを扱うことは稀で、基本的には少ないデータで始めて徐々に多くしていったり、精度向上のため変数の変更、追加、削除をすることがあるからだ。
しかし、このときデータその1を処理したスクリプトを一切変更せずその2を前処理できることは極めて稀だと言っていいと思う。具体的に何が起きるかを例で示そう。
具体例 かんたんのため小さいデータセットで書く。
    foo bar     1 1 2   2 3 4   3 5 6    こんなデータがあるとして foo 列と bar 列を標準化したい3とする。さっと書くと次のようなコードになる(上のデータが data1.csv に書き出されているものとする)。
import pandas as pd from sklearn.preprocessing import StandardScaler d = pd.read_csv(&#39;data1.csv&#39;) sc = StandardScaler() print(sc.fit_transform(d)) # 実行結果 # [[-1.</description>
    </item>
    
  </channel>
</rss>